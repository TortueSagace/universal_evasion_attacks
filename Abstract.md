# Metaheuristic-based evasion attacks against state-of-the-art near-perfect machine learning classifiers

This thesis presents a novel framework for evaluating and exploiting the adversarial robustness of machine learning classifiers through the design and implementation of versatile evasion attack algorithms. Recognizing that near-perfect classification accuracy does not guarantee resilience against carefully crafted perturbations, this work introduces adversarially adapted metaheuristics for efficiently navigating constrained feature spaces in both image and tabular domains. Central to our approach is the development of a new Python package, “Versatile Evasion Attacks” (VEA), which implements these metaheuristic strategies and incorporates realistic constraints (e.g., feature immutability and clipping) that mimic practical attack scenarios. To address the challenge of quantifying imperceptibility and attack efficacy in tabular data, where traditional L_2 norms are often inadequate, we propose a set of novel cost metrics. In particular, the Standard Linear Attribute Relative Cost (SLARC) and its absolute counterpart (SLAAC) are introduced to capture feature-specific importance and variability, while the Heuristic Adversarial Robustness (HAR) score provides a normalized measure of a model’s vulnerability to targeted evasion attacks. Extensive empirical studies are conducted on a range of datasets, including MNIST for image classification, a Credit Card Fraud dataset for financial applications, and two IoT cybersecurity datasets (CICIoT23 and N-BaIoT). These experiments reveal significant differences in the robustness of state-of-the-art models, underscoring that high nominal accuracy does not inherently imply security against adversarial manipulations. The contributions of this thesis include the advancement of metaheuristic attack methodologies, the introduction of statistically grounded cost functions for adversarial evaluation, and comprehensive empirical validations. Collectively, these findings offer valuable insights for the development of more secure machine learning systems in real-world, high-stakes applications.
